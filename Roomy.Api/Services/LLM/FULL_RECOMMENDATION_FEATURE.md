# ‚úÖ LLM gibt jetzt vollst√§ndige Empfehlung mit Begr√ºndung zur√ºck

## √Ñnderungen

### 1. Service-Signatur ge√§ndert

**Vorher:**
```csharp
public async Task<int?> GetRecommendedRoomIdAsync(...)
```
Gab nur die Raum-ID zur√ºck.

**Nachher:**
```csharp
public async Task<(int? roomId, string? recommendation)> GetRecommendedRoomAsync(...)
```
Gibt Tupel mit Raum-ID **und** vollst√§ndiger LLM-Empfehlung zur√ºck.

### 2. Prompt Engineering √ºberarbeitet

**System Prompt:**
```
Du bist ein intelligenter Assistent f√ºr Raumbuchungen. 
Deine Aufgabe ist es, basierend auf der Anfrage des Benutzers den am besten passenden Raum zu empfehlen.
Analysiere die Anforderungen (Kapazit√§t, Ausstattung, Typ) und w√§hle den optimalen Raum aus.

WICHTIG: Deine Antwort muss folgendes Format haben:
1. Erste Zeile: Nur der exakte Raumname (genau wie in der Liste angegeben)
2. Danach: Eine ausf√ºhrliche Empfehlung mit Ausstattung und Begr√ºndung

Beispiel:
Meeting Room A
Ich empfehle den Meeting Room A.
**Ausstattung:** [Beschreibe die Ausstattung]
**Begr√ºndung:** [Erkl√§re warum dieser Raum optimal ist]
```

**User Prompt:**
```
Verf√ºgbare R√§ume f√ºr morgen:
[Raumliste]

Benutzeranfrage: "[Anfrage]"

Welcher Raum passt am besten? Gib zuerst den exakten Raumnamen an, 
dann eine ausf√ºhrliche Empfehlung mit Ausstattung und Begr√ºndung.
```

### 3. Parsing-Logik

**Vorher:** Parse Integer (Raum-ID)
```csharp
if (int.TryParse(llmRecommendation, out int roomId))
{
    // Suche Raum nach ID
}
```

**Nachher:** Parse Raumname aus erster Zeile
```csharp
var lines = llmRecommendation.Split('\n', StringSplitOptions.RemoveEmptyEntries);
var recommendedRoomName = lines[0].Trim();

// Suche Raum nach Name (case-insensitive)
var recommendedRoom = availableRooms.FirstOrDefault(r => 
    r.Name.Equals(recommendedRoomName, StringComparison.OrdinalIgnoreCase));

if (recommendedRoom != null)
{
    return (recommendedRoom.Id, llmRecommendation); // Komplette Empfehlung
}
```

### 4. Response Model erweitert

**RecommendedRoomResponse:**
```csharp
public class RecommendedRoomResponse
{
    public string RoomName { get; set; }
    public int RoomId { get; set; }
    public int Capacity { get; set; }
    public string? Description { get; set; }
    
    // NEU: Vollst√§ndige LLM-Empfehlung mit Ausstattung und Begr√ºndung
    public string? LlmRecommendation { get; set; }
}
```

### 5. Endpoint aktualisiert

```csharp
var (llmRoomId, llmResponse) = await _llmService.GetRecommendedRoomAsync(
    availableRooms, req.Criteria, ct);

if (llmRoomId.HasValue)
{
    recommendedRoom = availableRooms.FirstOrDefault(r => r.Id == llmRoomId.Value);
    llmRecommendation = llmResponse; // Speichere komplette Empfehlung
}

Response = new RecommendedRoomResponse
{
    RoomName = recommendedRoom.Name,
    RoomId = recommendedRoom.Id,
    Capacity = recommendedRoom.Capacity,
    Description = recommendedRoom.Description,
    LlmRecommendation = llmRecommendation // Wird in Response zur√ºckgegeben
};
```

## Beispiel-Response

### Request:
```json
POST /api/rooms/recommended
{
  "criteria": "Ich m√∂chte einen Raum f√ºr 5 Leute mit Kaffeemaschine"
}
```

### Response:
```json
{
  "roomName": "Huddle Space",
  "roomId": 3,
  "capacity": 4,
  "description": "Kleiner Raum f√ºr informelle Diskussionen",
  "llmRecommendation": "Huddle Space\nIch empfehle den **Huddle Space**.\n\n**Ausstattung:** Der Huddle Space ist ein kleiner Raum, der f√ºr bis zu 4 Personen ausgelegt ist und ideal f√ºr informelle Diskussionen ist. Leider ist die Kaffeemaschine nicht in der Beschreibung erw√§hnt, aber er ist der am besten geeignete Raum f√ºr eine kleine Gruppe.\n\n**Begr√ºndung:** Obwohl der Huddle Space nur f√ºr 4 Personen ausgelegt ist, ist er der einzige verf√ºgbare Raum, der f√ºr eine kleine Gruppe von 5 Personen am n√§chsten kommt. F√ºr eine Gruppe von 5 Personen k√∂nnte man eventuell eine Person etwas enger setzen oder den Raum f√ºr eine informelle Besprechung nutzen. Leider gibt es keinen Raum, der genau f√ºr 5 Personen geeignet ist und eine Kaffeemaschine bietet. Wenn die Kaffeemaschine ein Muss ist, w√§re es sinnvoll, dies bei der Buchung zu kl√§ren oder nach einem anderen Raum zu suchen, der diese Ausstattung hat."
}
```

## LLM-Antwort Format

Das LLM gibt jetzt eine strukturierte Antwort zur√ºck:

```
[Raumname]
Ich empfehle den **[Raumname]**.

**Ausstattung:** [Detaillierte Beschreibung der Ausstattung]

**Begr√ºndung:** [Ausf√ºhrliche Erkl√§rung warum dieser Raum optimal ist]
```

### Beispiel:
```
Huddle Space
Ich empfehle den **Huddle Space**.

**Ausstattung:** Der Huddle Space ist ein kleiner Raum, der f√ºr bis zu 4 Personen 
ausgelegt ist und ideal f√ºr informelle Diskussionen ist. Leider ist die Kaffeemaschine 
nicht in der Beschreibung erw√§hnt, aber er ist der am besten geeignete Raum f√ºr eine 
kleine Gruppe.

**Begr√ºndung:** Obwohl der Huddle Space nur f√ºr 4 Personen ausgelegt ist, ist er der 
einzige verf√ºgbare Raum, der f√ºr eine kleine Gruppe von 5 Personen am n√§chsten kommt. 
F√ºr eine Gruppe von 5 Personen k√∂nnte man eventuell eine Person etwas enger setzen oder 
den Raum f√ºr eine informelle Besprechung nutzen...
```

## Vorteile

‚úÖ **Transparenz**: Benutzer sieht die Begr√ºndung des LLM
‚úÖ **Vertrauen**: Nachvollziehbare Entscheidungen
‚úÖ **Kontext**: Informationen √ºber Ausstattung und Einschr√§nkungen
‚úÖ **Intelligenz**: LLM kann komplexe Abw√§gungen erkl√§ren
‚úÖ **Flexibilit√§t**: Auch wenn kein perfekter Match, erkl√§rt LLM Alternativen

## Logging

Das Logging zeigt jetzt:
```
[INFO] LLM Empfehlung (roh): Huddle Space
Ich empfehle den **Huddle Space**...
[INFO] LLM hat Raum empfohlen: Huddle Space
[INFO] ‚úì Raum 'Huddle Space' (ID: 3) gefunden
[INFO] === LLM Empfehlung erfolgreich ===
```

## Frontend-Integration

Das Frontend kann jetzt die `llmRecommendation` anzeigen:

```typescript
interface RecommendedRoomResponse {
  roomName: string;
  roomId: number;
  capacity: number;
  description?: string;
  llmRecommendation?: string; // NEU: Markdown-formatierter Text
}
```

Anzeige mit Markdown-Rendering:
```tsx
{response.llmRecommendation && (
  <div className="llm-recommendation">
    <h3>KI-Empfehlung</h3>
    <ReactMarkdown>{response.llmRecommendation}</ReactMarkdown>
  </div>
)}
```

## Testing

### Manuelle Tests

**Test 1: Spezifische Anforderungen**
```bash
POST /api/rooms/recommended
{
  "criteria": "Ich m√∂chte einen Raum f√ºr 5 Leute mit Kaffeemaschine"
}
```

**Test 2: Nat√ºrlichsprachlich**
```bash
POST /api/rooms/recommended
{
  "criteria": "Wir brauchen einen Raum mit Beamer f√ºr eine Pr√§sentation, ca. 8 Personen"
}
```

**Test 3: Komplexe Anforderungen**
```bash
POST /api/rooms/recommended
{
  "criteria": "Gro√üer Konferenzraum mit Videokonferenz-Equipment f√ºr 20 Leute"
}
```

## Status

‚úÖ **Service**: Gibt Tupel (ID, Empfehlung) zur√ºck
‚úÖ **Prompt**: Strukturierte Antwort mit Raumname + Begr√ºndung
‚úÖ **Parsing**: Extrahiert Raumname aus erster Zeile
‚úÖ **Response**: Enth√§lt vollst√§ndige LLM-Empfehlung
‚úÖ **Logging**: Zeigt Raumname und komplette Empfehlung
‚úÖ **Fallback**: Regelbasierte Logik wenn LLM fehlschl√§gt

Die LLM-Integration gibt jetzt aussagekr√§ftige, begr√ºndete Empfehlungen zur√ºck! üéâ

